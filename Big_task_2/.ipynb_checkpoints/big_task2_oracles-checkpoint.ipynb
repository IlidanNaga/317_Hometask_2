{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BaseSmoothOracle:\n",
    "    \"\"\"\n",
    "    Базовый класс для реализации оракулов.\n",
    "    \"\"\"\n",
    "    def func(self, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение функции в точке w.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Func oracle is not implemented.')\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение градиента функции в точке w.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Grad oracle is not implemented.')\n",
    "\n",
    "        \n",
    "class BinaryLogistic(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Оракул для задачи двухклассовой логистической регрессии.\n",
    "    \n",
    "    Оракул должен поддерживать l2 регуляризацию.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, l2_coef):\n",
    "        \"\"\"\n",
    "        Задание параметров оракула.\n",
    "        \n",
    "        l2_coef - коэффициент l2 регуляризации\n",
    "        \"\"\"\n",
    "        self.l2 = l2_coef / 2\n",
    "     \n",
    "    def func(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение функционала в точке w на выборке X с ответами y.\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        w - одномерный numpy array\n",
    "        \n",
    "        \"\"\"\n",
    "        # логистическая функция потерь\n",
    "        \n",
    "        M = y * X.dot(w)\n",
    "        \n",
    "        loss = np.mean(np.log(1 + np.exp(-1 * M)))\n",
    "        loss += self.l2  * np.dot(w, w)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def grad(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Вычислить градиент функционала в точке w на выборке X с ответами y.\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        w - одномерный numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(type(X), X.shape, X)\n",
    "        #print(type(y), y.shape, y)\n",
    "        #print(type(w), w.shape, w)\n",
    "        \n",
    "        M = y * X.dot(w)\n",
    "        \n",
    "        grad = 1 / (1 + np.exp(M))\n",
    "        \n",
    "        grad = np.mean(-1 * grad * y * X.T, axis=1) + 2 * self.l2 * w\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time\n",
    "import random\n",
    "\n",
    "class GDClassifier:\n",
    "    \"\"\"\n",
    "    Реализация метода градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_function, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "                \n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "        \n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "        \n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию.\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "        \n",
    "        max_iter - максимальное число итераций     \n",
    "        \n",
    "        **kwargs - аргументы, необходимые для инициализации   \n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.l2 = kwargs['l2_coef']\n",
    "        \n",
    "        self.oracle = BinaryLogistic\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, w_0=None, trace=False):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        w_0 - начальное приближение в методе\n",
    "        \n",
    "        trace - переменная типа bool\n",
    "      \n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Длина словаря history = количество итераций + 1 (начальное приближение)\n",
    "        \n",
    "        history['time']: list of floats, содержит интервалы времени между двумя итерациями метода\n",
    "        history['func']: list of floats, содержит значения функции на каждой итерации\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        history = {}\n",
    "        history['time'] = list()\n",
    "        history['func'] = list()\n",
    "        \n",
    "        w_new = w_0\n",
    "        w_old = w_0\n",
    "        \n",
    "        start = stop = 0\n",
    "        \n",
    "        history['func'].append(w_0)\n",
    "        history['time'].append(0)\n",
    "        \n",
    "        for step in range(1, self.max_iter + 1):\n",
    "            start = time.time()\n",
    "            #print(X.shape, y.shape, w_old.shape)\n",
    "            #print(self.oracle.grad(self, X, y, w_old))\n",
    "            w_new = w_old - (self.step_alpha / step ** self.step_beta) * self.oracle.grad(self, X, y, w_old)\n",
    "            history['func'].append(w_new)\n",
    "            if np.abs(self.oracle.func(X, y, w_new) - self.oracle.func(X, y, w_old)) < self.tolerance:\n",
    "                break\n",
    "            w_old = w_new\n",
    "            stop = time.time()\n",
    "            history['time'].append(stop - start)\n",
    "            \n",
    "            \n",
    "        self.w = w_new\n",
    "        if self.trace:\n",
    "            return history\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Получение меток ответов на выборке X\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        return: одномерный numpy array с предсказаниями\n",
    "        \"\"\"\n",
    "        proba = (1 + np.exp(-1 * X.dot(w))) ** -1\n",
    "        answer = np.ones_like(proba)\n",
    "        answer[proba < 0.5] = -1\n",
    "        return answer\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Получение вероятностей принадлежности X к классу k\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        return: двумерной numpy array, [i, k] значение соответветствует вероятности\n",
    "        принадлежности i-го объекта к классу k \n",
    "        \n",
    "        \"\"\"\n",
    "        proba = (1 + np.exp(-1 * X.dot(w))) ** -1\n",
    "        answer = np.empty(2 * len(proba)).reshape(-1, 2)\n",
    "        answer[:,0] = proba\n",
    "        answer[:,1] = 1 - proba\n",
    "        return answer\n",
    "        \n",
    "    def get_objective(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения целевой функции на выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        return: float\n",
    "        \"\"\"\n",
    "        return self.oracle.func(X, y, self.w)\n",
    "        \n",
    "    def get_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения градиента функции на выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        return: numpy array, размерность зависит от задачи\n",
    "        \"\"\"\n",
    "        return self.oracle.grad(X, y, self.w)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Получение значения весов функционала\n",
    "        \"\"\"    \n",
    "        return self.w\n",
    "\n",
    "\n",
    "\n",
    "class SGDClassifier(GDClassifier):\n",
    "    \"\"\"\n",
    "    Реализация метода стохастического градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_function, batch_size, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, random_seed=153, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "        \n",
    "        batch_size - размер подвыборки, по которой считается градиент\n",
    "        \n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "        \n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "        \n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "        \n",
    "        \n",
    "        max_iter - максимальное число итераций (эпох)\n",
    "        \n",
    "        random_seed - в начале метода fit необходимо вызвать np.random.seed(random_seed).\n",
    "        Этот параметр нужен для воспроизводимости результатов на разных машинах.\n",
    "        \n",
    "        **kwargs - аргументы, необходимые для инициализации\n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.seed = random_seed\n",
    "        self.l2 = kwargs[0]\n",
    "        \n",
    "        self.oracle = BinaryLogostic\n",
    "        \n",
    "    def fit(self, X, y, w_0=None, trace=False, log_freq=1):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "                \n",
    "        w_0 - начальное приближение в методе\n",
    "        \n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Если обновлять history после каждой итерации, метод перестанет \n",
    "        превосходить в скорости метод GD. Поэтому, необходимо обновлять историю метода лишь\n",
    "        после некоторого числа обработанных объектов в зависимости от приближённого номера эпохи.\n",
    "        Приближённый номер эпохи:\n",
    "            {количество объектов, обработанных методом SGD} / {количество объектов в выборке}\n",
    "        \n",
    "        log_freq - float от 0 до 1, параметр, отвечающий за частоту обновления. \n",
    "        Обновление должно проиходить каждый раз, когда разница между двумя значениями приближённого номера эпохи\n",
    "        будет превосходить log_freq.\n",
    "        \n",
    "        history['epoch_num']: list of floats, в каждом элементе списка будет записан приближённый номер эпохи:\n",
    "        history['time']: list of floats, содержит интервалы времени между двумя соседними замерами\n",
    "        history['func']: list of floats, содержит значения функции после текущего приближённого номера эпохи\n",
    "        history['weights_diff']: list of floats, содержит квадрат нормы разности векторов весов с соседних замеров\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        history = {}\n",
    "        history['time'] = list()\n",
    "        history['func'] = list()\n",
    "        \n",
    "        w_new = w_0\n",
    "        w_old = w_0\n",
    "        \n",
    "        start = stop = 0\n",
    "        \n",
    "        history['func'].append(w_0)\n",
    "        history['time'].append(0)\n",
    "        \n",
    "        for step in range(1, self.max_iter + 1):\n",
    "            start = time.time()\n",
    "            ind = random.randint()\n",
    "            w_new = w_old - (self.step_alpha / step ** self.step_beta) * self.oracle.grad( X, y, w_old)\n",
    "            history['func'].append(w_new)\n",
    "            if np.abs(self.oracle.func(X, y, w_new) - self.oracle.func(X, y, w_old)) < self.tolerance:\n",
    "                break\n",
    "            w_old = w_new\n",
    "            stop = time.time()\n",
    "            history['time'].append(stop - start)\n",
    "            \n",
    "            \n",
    "        self.w = w_new\n",
    "        if self.trace:\n",
    "            return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "clf = GDClassifier(loss_function='binary_logistic', step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1)\n",
    "l, d = 1000, 10\n",
    "X = np.random.random((l, d))\n",
    "y = np.random.randint(0, 2, l) * 2 - 1\n",
    "w = np.random.random(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,) (10,)\n",
      "<class 'numpy.ndarray'> (1000, 10) [[0.77132064 0.02075195 0.63364823 ... 0.76053071 0.16911084 0.08833981]\n",
      " [0.68535982 0.95339335 0.00394827 ... 0.29187607 0.91777412 0.71457578]\n",
      " [0.54254437 0.14217005 0.37334076 ... 0.51313824 0.65039718 0.60103895]\n",
      " ...\n",
      " [0.28560093 0.04180989 0.311687   ... 0.87520415 0.59685479 0.18183317]\n",
      " [0.9860008  0.56312705 0.4198946  ... 0.15757419 0.69297496 0.55246966]\n",
      " [0.22860332 0.01412775 0.84632447 ... 0.0050953  0.3826363  0.89368739]]\n",
      "<class 'numpy.ndarray'> (1000,) [ 1 -1 -1  1  1  1 -1  1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1 -1  1 -1\n",
      " -1  1  1 -1  1  1  1 -1 -1 -1 -1  1 -1  1 -1  1 -1  1  1 -1  1  1  1 -1\n",
      " -1 -1 -1 -1 -1  1 -1  1  1  1 -1 -1  1  1 -1  1  1 -1 -1  1  1 -1  1  1\n",
      "  1 -1  1 -1 -1 -1  1 -1  1 -1  1  1 -1 -1  1  1  1  1 -1  1 -1  1  1 -1\n",
      "  1  1 -1 -1  1  1 -1  1 -1 -1  1  1 -1  1 -1  1  1  1  1 -1  1 -1  1 -1\n",
      "  1 -1  1 -1  1 -1  1 -1  1 -1 -1 -1  1  1 -1 -1  1  1 -1  1 -1  1 -1 -1\n",
      " -1 -1 -1  1  1 -1  1 -1 -1  1  1 -1  1 -1  1  1 -1 -1 -1  1  1 -1  1 -1\n",
      "  1  1 -1 -1  1  1  1  1 -1 -1  1 -1  1 -1 -1  1  1  1  1  1  1 -1  1  1\n",
      " -1  1  1 -1 -1  1 -1 -1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1  1 -1  1 -1\n",
      " -1  1 -1  1  1 -1  1 -1  1 -1 -1  1  1  1  1 -1 -1  1 -1  1 -1  1 -1  1\n",
      "  1  1  1  1  1 -1 -1 -1 -1  1  1 -1  1  1 -1 -1  1 -1 -1  1 -1  1 -1  1\n",
      "  1 -1 -1  1  1  1 -1  1  1 -1  1  1 -1 -1  1  1 -1  1 -1  1 -1 -1  1 -1\n",
      "  1  1 -1 -1 -1  1 -1  1 -1  1  1  1  1  1 -1 -1  1 -1 -1  1 -1  1  1  1\n",
      "  1 -1  1  1 -1 -1  1  1  1  1  1 -1 -1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1\n",
      "  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1 -1  1 -1 -1  1  1 -1 -1\n",
      "  1 -1 -1  1  1  1 -1  1 -1 -1 -1 -1  1 -1 -1  1 -1 -1  1  1  1  1  1  1\n",
      " -1  1 -1 -1 -1  1 -1 -1  1  1  1  1  1 -1 -1  1 -1 -1 -1 -1 -1 -1  1 -1\n",
      " -1 -1 -1 -1 -1  1  1  1 -1 -1 -1  1 -1  1  1  1  1  1 -1  1  1  1 -1 -1\n",
      "  1 -1 -1 -1  1  1  1 -1  1  1 -1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  1 -1 -1 -1  1  1  1  1  1 -1 -1 -1  1 -1  1  1  1  1 -1  1  1\n",
      " -1  1  1 -1  1 -1 -1  1 -1 -1  1  1 -1  1 -1  1  1 -1  1 -1  1  1 -1 -1\n",
      " -1  1 -1  1  1  1  1 -1 -1 -1  1  1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  1  1 -1  1 -1 -1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1 -1  1 -1 -1\n",
      " -1 -1 -1  1  1  1 -1  1 -1  1 -1  1 -1  1  1  1 -1 -1  1  1  1  1  1  1\n",
      "  1 -1  1 -1  1 -1  1  1  1  1 -1 -1 -1  1 -1  1  1 -1 -1  1  1 -1 -1  1\n",
      "  1 -1 -1 -1 -1 -1  1  1  1  1  1 -1  1  1 -1 -1 -1  1 -1 -1  1  1  1 -1\n",
      "  1 -1 -1  1 -1  1 -1  1  1  1 -1 -1 -1  1 -1  1  1 -1  1  1  1  1  1  1\n",
      " -1 -1 -1 -1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1 -1 -1  1 -1 -1  1 -1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1 -1 -1  1  1  1  1  1 -1 -1 -1  1 -1 -1  1\n",
      "  1  1 -1 -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1  1  1 -1  1  1 -1  1 -1 -1 -1\n",
      " -1 -1  1 -1  1 -1 -1  1  1  1  1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1 -1  1\n",
      " -1  1 -1  1 -1  1 -1 -1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1 -1  1  1 -1\n",
      " -1 -1 -1  1 -1 -1  1 -1 -1  1 -1 -1 -1  1 -1  1  1  1 -1  1 -1 -1 -1  1\n",
      " -1 -1  1  1 -1 -1  1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1  1  1  1  1 -1 -1\n",
      "  1 -1 -1 -1  1  1 -1 -1 -1  1  1  1  1 -1 -1  1  1 -1 -1 -1  1  1  1 -1\n",
      "  1 -1  1  1  1  1 -1 -1  1 -1 -1 -1  1  1  1  1  1 -1 -1  1  1  1 -1 -1\n",
      " -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1  1  1 -1  1  1 -1 -1  1  1  1  1\n",
      " -1  1  1  1 -1 -1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1 -1  1  1\n",
      "  1  1  1  1  1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1  1 -1\n",
      "  1  1  1 -1  1 -1 -1 -1  1  1 -1  1 -1 -1  1  1 -1 -1 -1  1 -1 -1 -1  1\n",
      "  1  1 -1 -1  1 -1 -1 -1  1 -1 -1  1 -1  1 -1  1 -1  1  1 -1  1  1 -1 -1\n",
      " -1  1  1  1 -1 -1 -1  1 -1 -1  1  1 -1  1 -1  1]\n",
      "<class 'numpy.ndarray'> (10,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 0.00027869  0.00711611  0.01160488 -0.00086325  0.00926855  0.00675723\n",
      "  0.00278599  0.00450789  0.01230398  0.01148879]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "grad() missing 1 required positional argument: 'w'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3cdfb0d21e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'func'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-08e39b209b4e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, w_0, trace)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mw_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_alpha\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_beta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'func'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_new\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: grad() missing 1 required positional argument: 'w'"
     ]
    }
   ],
   "source": [
    "\n",
    "history = clf.fit(X, y, w_0=np.zeros(d), trace=True)\n",
    "print(' '.join([str(x) for x in history['func']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
