{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BaseSmoothOracle:\n",
    "    \"\"\"\n",
    "    Базовый класс для реализации оракулов.\n",
    "    \"\"\"\n",
    "    def func(self, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение функции в точке w.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Func oracle is not implemented.')\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение градиента функции в точке w.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Grad oracle is not implemented.')\n",
    "\n",
    "        \n",
    "class BinaryLogistic(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Оракул для задачи двухклассовой логистической регрессии.\n",
    "    \n",
    "    Оракул должен поддерживать l2 регуляризацию.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, l2_coef):\n",
    "        \"\"\"\n",
    "        Задание параметров оракула.\n",
    "        \n",
    "        l2_coef - коэффициент l2 регуляризации\n",
    "        \"\"\"\n",
    "        self.l2 = l2_coef / 2\n",
    "     \n",
    "    def func(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Вычислить значение функционала в точке w на выборке X с ответами y.\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        w - одномерный numpy array\n",
    "        \n",
    "        \"\"\"\n",
    "        # логистическая функция потерь\n",
    "        \n",
    "        M = y * X.dot(w)\n",
    "        \n",
    "        loss = np.mean(np.log(1 + np.exp(-1 * M)))\n",
    "        loss += self.l2  * np.dot(w, w)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def grad(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Вычислить градиент функционала в точке w на выборке X с ответами y.\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        w - одномерный numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(type(X), X.shape, X)\n",
    "        #print(type(y), y.shape, y)\n",
    "        #print(type(w), w.shape, w)\n",
    "        \n",
    "        M = y * X.dot(w)\n",
    "        \n",
    "        grad = 1 / (1 + np.exp(M))\n",
    "        \n",
    "        grad = np.mean(-1 * grad * y * X.T, axis=1) + 2 * self.l2 * w\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time\n",
    "import random\n",
    "\n",
    "class GDClassifier:\n",
    "    \"\"\"\n",
    "    Реализация метода градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_function, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "                \n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "        \n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "        \n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию.\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "        \n",
    "        max_iter - максимальное число итераций     \n",
    "        \n",
    "        **kwargs - аргументы, необходимые для инициализации   \n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.l2 = kwargs['l2_coef']\n",
    "        \n",
    "        self.oracle = BinaryLogistic\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, w_0=None, trace=False):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        w_0 - начальное приближение в методе\n",
    "        \n",
    "        trace - переменная типа bool\n",
    "      \n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Длина словаря history = количество итераций + 1 (начальное приближение)\n",
    "        \n",
    "        history['time']: list of floats, содержит интервалы времени между двумя итерациями метода\n",
    "        history['func']: list of floats, содержит значения функции на каждой итерации\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        history = {}\n",
    "        history['time'] = list()\n",
    "        history['func'] = list()\n",
    "        \n",
    "        w_new = w_0\n",
    "        w_old = w_0\n",
    "        \n",
    "        start = stop = 0\n",
    "        \n",
    "        history['func'].append(w_0)\n",
    "        history['time'].append(0)\n",
    "        \n",
    "        for step in range(1, self.max_iter + 1):\n",
    "            start = time.time()\n",
    "            #print(X.shape, y.shape, w_old.shape)\n",
    "            #print(self.oracle.grad(self, X, y, w_old))\n",
    "            w_new = w_old - (self.step_alpha / step ** self.step_beta) * self.oracle.grad(self, X, y, w_old)\n",
    "            history['func'].append(w_new)\n",
    "            if np.abs(self.oracle.func(self, X, y, w_new) - self.oracle.func(self, X, y, w_old)) < self.tolerance:\n",
    "                break\n",
    "            w_old = w_new\n",
    "            stop = time.time()\n",
    "            history['time'].append(stop - start)\n",
    "            \n",
    "            \n",
    "        self.w = w_new\n",
    "        if trace:\n",
    "            return history\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Получение меток ответов на выборке X\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        return: одномерный numpy array с предсказаниями\n",
    "        \"\"\"\n",
    "        proba = (1 + np.exp(-1 * X.dot(w))) ** -1\n",
    "        answer = np.ones_like(proba)\n",
    "        answer[proba < 0.5] = -1\n",
    "        return answer\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Получение вероятностей принадлежности X к классу k\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        return: двумерной numpy array, [i, k] значение соответветствует вероятности\n",
    "        принадлежности i-го объекта к классу k \n",
    "        \n",
    "        \"\"\"\n",
    "        proba = (1 + np.exp(-1 * X.dot(w))) ** -1\n",
    "        answer = np.empty(2 * len(proba)).reshape(-1, 2)\n",
    "        answer[:,0] = proba\n",
    "        answer[:,1] = 1 - proba\n",
    "        return answer\n",
    "        \n",
    "    def get_objective(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения целевой функции на выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        return: float\n",
    "        \"\"\"\n",
    "        return self.oracle.func(X, y, self.w)\n",
    "        \n",
    "    def get_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения градиента функции на выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        return: numpy array, размерность зависит от задачи\n",
    "        \"\"\"\n",
    "        return self.oracle.grad(X, y, self.w)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Получение значения весов функционала\n",
    "        \"\"\"    \n",
    "        return self.w\n",
    "\n",
    "\n",
    "\n",
    "class SGDClassifier(GDClassifier):\n",
    "    \"\"\"\n",
    "    Реализация метода стохастического градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_function, batch_size, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, random_seed=153, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "        \n",
    "        batch_size - размер подвыборки, по которой считается градиент\n",
    "        \n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "        \n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "        \n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "        \n",
    "        \n",
    "        max_iter - максимальное число итераций (эпох)\n",
    "        \n",
    "        random_seed - в начале метода fit необходимо вызвать np.random.seed(random_seed).\n",
    "        Этот параметр нужен для воспроизводимости результатов на разных машинах.\n",
    "        \n",
    "        **kwargs - аргументы, необходимые для инициализации\n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.seed = random_seed\n",
    "        self.l2 = kwargs[0]\n",
    "        \n",
    "        self.oracle = BinaryLogostic\n",
    "        \n",
    "    def fit(self, X, y, w_0=None, trace=False, log_freq=1):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "                \n",
    "        w_0 - начальное приближение в методе\n",
    "        \n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Если обновлять history после каждой итерации, метод перестанет \n",
    "        превосходить в скорости метод GD. Поэтому, необходимо обновлять историю метода лишь\n",
    "        после некоторого числа обработанных объектов в зависимости от приближённого номера эпохи.\n",
    "        Приближённый номер эпохи:\n",
    "            {количество объектов, обработанных методом SGD} / {количество объектов в выборке}\n",
    "        \n",
    "        log_freq - float от 0 до 1, параметр, отвечающий за частоту обновления. \n",
    "        Обновление должно проиходить каждый раз, когда разница между двумя значениями приближённого номера эпохи\n",
    "        будет превосходить log_freq.\n",
    "        \n",
    "        history['epoch_num']: list of floats, в каждом элементе списка будет записан приближённый номер эпохи:\n",
    "        history['time']: list of floats, содержит интервалы времени между двумя соседними замерами\n",
    "        history['func']: list of floats, содержит значения функции после текущего приближённого номера эпохи\n",
    "        history['weights_diff']: list of floats, содержит квадрат нормы разности векторов весов с соседних замеров\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        history = {}\n",
    "        history['time'] = list()\n",
    "        history['func'] = list()\n",
    "        \n",
    "        w_new = w_0\n",
    "        w_old = w_0\n",
    "        \n",
    "        start = stop = 0\n",
    "        \n",
    "        history['func'].append(w_0)\n",
    "        history['time'].append(0)\n",
    "        \n",
    "        for step in range(1, self.max_iter + 1):\n",
    "            start = time.time()\n",
    "            ind = random.randint()\n",
    "            w_new = w_old - (self.step_alpha / step ** self.step_beta) * self.oracle.grad( X, y, w_old)\n",
    "            history['func'].append(w_new)\n",
    "            if np.abs(self.oracle.func(X, y, w_new) - self.oracle.func(X, y, w_old)) < self.tolerance:\n",
    "                break\n",
    "            w_old = w_new\n",
    "            stop = time.time()\n",
    "            history['time'].append(stop - start)\n",
    "            \n",
    "            \n",
    "        self.w = w_new\n",
    "        if self.trace:\n",
    "            return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "clf = GDClassifier(loss_function='binary_logistic', step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1)\n",
    "l, d = 1000, 10\n",
    "X = np.random.random((l, d))\n",
    "y = np.random.randint(0, 2, l) * 2 - 1\n",
    "w = np.random.random(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [-0.00027869 -0.00711611 -0.01160488  0.00086325 -0.00926855 -0.00675723\n",
      " -0.00278599 -0.00450789 -0.01230398 -0.01148879] [ 0.0033826  -0.00851303 -0.01661495  0.0055345  -0.01241381 -0.00809236\n",
      " -0.00112568 -0.00418905 -0.01786489 -0.01644842] [ 0.00690655 -0.00888161 -0.01981649  0.00986344 -0.01414834 -0.00844196\n",
      "  0.00082816 -0.00327148 -0.02149638 -0.01960599]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = clf.fit(X, y, w_0=np.zeros(d), trace=True)\n",
    "print(' '.join([str(x) for x in history['func']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
